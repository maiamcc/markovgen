(Abandoned) Markov Generator
============================

I started this project at Hacker School in summer '14. It fell by the wayside when I realized my grand plans for a part-of-speech-based Markov text generator didn't actually make any sense. Here's the documentation for all of the existing code, in all its glory, so that I can pick up on this again one day.

What This Repo Contains
-----------------------
*markovgen.py* - the main code file. Contains the classes Markov and POS_Markov, which take an input file (the _corpus_) and generate random text with varying degrees of comprehensibility.

*corpuscleaner.py* - when beginning to work with a new corpus, run the text file through corpuscleaner.py. This will remove all numbers, the words "chapter" and "book", and any additional strings specified by the user in the file.

*corpus_tagger_cpickle.py* - part-of-speech tags a given corpus text file with the [Natural Language Toolkit's](http://www.nltk.org/) part-of-speech tagger, then saves the result in cPickle.

*/texts/plain* - contains a number of possible corpuses as plain text files.

*/texts/tagged* - contains a number of possible corpuses part-of-speech tagged with [nltk](http://www.nltk.org/) and encoded with cPickle.

How to Use the Code
-------------------

### Class: Markov

Make a new Markov gen with an argument of the plaintext file of your corpus.

`mymarkov = Markov("texts/plain/mycorpus.txt")`

Upon creation, it will automatically populate a dictionary with word tri-grams; for every three consecutive words in the text, the first two will key to the third.

Then, you can run the generate function to generate a text of a given length in words (default length is 100).

`mymarkov.generate(250)` -- generates a random text of 250 words

### Class: POS_Markov

Make a new part-of-speech-based Markov gen with an argument of the cPickled, tagged file of your corpus (use *corpustagger.py* and *corpus_tagger_cpickle.py*, mentioned above).

`mymarkov = Markov("texts/tagged/mycorpus_cPickle.txt")`

Upon creation, it will automatically populate three dictionaries. The _word dictionary_ uses tri-grams, as discussed above, and records ("word", "POS") tuples. The _pos dictionary_ only records POS's (again using tri-grams). The taga dictionary with word tri-grams; for every three consecutive words in the text, the first two will key to the third. The _tag dictionary_ keeps track of all of the words in the text indexed by part-of-speech.

Then, you can run the generate function to enerate a text of a given length in words (default length is 100). This function goes in a few steps.

First, like the plain Markov gen generates text, it uses the pos dictionary to generate an abstract chunk of text that consists only of parts-of-speech. (This is a really silly way to do things, but I didn't realize that at the time.) Then, the generate function attempts to fill in the abstract text chunk. Given the last two words of the output (w1, w2), if there exists a word in the word dictionary that might follow w1 and w2 AND is of the appropriate part of speech, that word will be added to the output. (If there is more than one option, the program will choose randomly.) If there are no words of the appropriate POS that could follow w1 and w2, OR there are no words at all that could follow w1 and w2 (i.e. the pair (w1, w2) isn't even a key in the word dictionary), the program will instead add a random word of the appropriate part of speech, as chosen from the tag dictionary.

This class CAN generate random text. It's just even more random and nonsensical than the texts generated by the plain-text Markov gen.

Future Steps/Ideas
------------------

- Rather than generate a POS-abstract chunk of text and fill it in, generate next POS and next word more-or-less concurrently; then, if there doesn't exist a word of the appropriate POS, rather than adding a random word, choose a different POS.

- The above would work, but seems like it would be little better than using tri-grams alone. Consider tracking POS on a larger scale for more grammatical coherance: maybe 5-grams, in conjunction with word tri-grams.

- Optimization: weight dictionary values more intelligently, instead of putting a value in the list multiple times corresponding to frequency.

- Fun experiment: run plain-text Markov gen with increasing n-grams of increasing n; find the line between coherance and reproducing the corpus.
